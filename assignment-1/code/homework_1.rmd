---
title: "Homework 1"
author: "Attila Szuts"
date: "2/16/2021"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    number_sections: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	cache = T
)
options("digits" = 5)
```

```{r}
rm(list = ls())
```


```{r libraries}
library(tidyverse)
library(datasets)
library(MASS)
library(ISLR)
library(caret)
library(skimr)
library(GGally)
library(kableExtra)
library(data.table)

source("assignment-1/code/helper.R")
```

# Supervised learning with penalized models and PCA

```{r load-data}
# load data
data <- readRDS(url('http://www.jaredlander.com/data/manhattan_Train.rds')) %>%
  mutate(logTotalValue = log(TotalValue)) %>%
  drop_na()
```


```{r data-prep}
# mutate to factor
data <- data %>% 
  mutate(
    Council = as.factor(Council),
    PolicePrct = as.factor(PolicePrct),
    HealthArea = as.factor(HealthArea)
  )
# skim(data)
```


```{r plots, echo=FALSE, fig.width=10, fig.height=10}
# set ggplot theme
theme_set(theme_bw())

# there are a lot of correlations with logTotalValue, however there are some correlation between predictors, too! penalized models would be great here
ggcorr(data)
```


```{r plots-2, include=FALSE}
# look at histograms
pl_hist_totalvalue <- data %>% 
  ggplot(aes(TotalValue)) + geom_histogram()

pl_hist_logtotalvalue <- data %>% 
  ggplot(aes(logTotalValue)) + geom_histogram()


# check boxplots
pl_box_zonedist <- box_fun(data$ZoneDist1)
pl_box_ownertype <- box_fun(data$OwnerType)
pl_box_landmark <- box_fun(data$Landmark)
pl_box_historicdist <- box_fun(data$HistoricDistrict)
pl_box_schooldist <- box_fun(data$SchoolDistrict)
pl_box_built <- box_fun(data$Built)
pl_box_council <- box_fun(data$Council)
pl_box_police <- box_fun(data$PolicePrct)

# check scatterplots
pl_scatter_comarea <- scatter_fun(data$ComArea)
pl_scatter_facilfar <- scatter_fun(data$FacilFAR)

```

## Setup model building

- Create train test split

```{r train-test-split}
set.seed(1234)
training_ratio <- 0.3
train_indices <- createDataPartition(
  y = data[["logTotalValue"]],
  times = 1,
  p = training_ratio,
  list = FALSE
) %>% as.vector()
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

- Find best predictors

```{r best-predictors}
# look at correlation between logTotalValue and predictors
corr <- data.frame(
  "logTotalValue" = 
    cor(
      y = data$logTotalValue, 
      x = dplyr::select(data, -logTotalValue & -TotalValue & where(is.numeric))
      )
  )

# move rownames to a column
corr$predictors <- row.names(corr)
row.names(corr) <- NULL

# find 10 best predictors
best_preds <- corr %>% 
  dplyr::select(predictors, logTotalValue) %>% 
  arrange(desc(logTotalValue)) %>% 
  top_n(10) %>% 
  dplyr::select(predictors)

all_preds <- corr$predictors
best_preds <- best_preds$predictors 
```


- Create formulas

```{r formulas}
# simple regression with the top correlating predictors
formula_reg1 <- formula(paste0(c("logTotalValue", paste0(c(best_preds), collapse = " + ")), collapse = " ~ "))
# extended regression with all the predictors
formula_reg2 <- formula(paste0(c("logTotalValue", paste0(c(all_preds), collapse = " + ")), collapse = " ~ "))
```

- Init results table

```{r}
results_list <- list()
```


## Model building 1.

First we are going to just build two regression models, a LASSO, a Ridge, and an Elastic net model. One regression model will use only the 10 best predictors, while the other will use all.

```{r train-control-1}
# set crossvalidation
trctrl <- trainControl(
  method = "cv",
  number = 10
)

# init list of models
models <- list()

```

```{r model-building-1}
# Regression with only the best predictors
set.seed(123)
model_reg1 <- train(
  formula_reg1,
  data = data_train,
  method = "lm",
  trControl = trctrl
)

models[["reg_simple"]] <- model_reg1

# Extended regression
set.seed(123)
model_reg2 <- train(
  formula_reg2,
  data = data_train,
  method = "lm",
  trControl = trctrl
)

models[["reg_extended"]] <- model_reg2

# LASSO
set.seed(123)

lasso_grid <- expand.grid(
  "alpha" = 1,
  "lambda" = seq(0, 1, by = 0.1)
)

model_lasso <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = lasso_grid
)

models[["lasso"]] <- model_lasso

# Ridge
set.seed(123)

ridge_grid <- expand.grid(
  "alpha" = 0,
  "lambda" = seq(0, 1, by = 0.1)
)

model_ridge <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = ridge_grid
)

models[["ridge"]] <- model_ridge

# Elastic Net
set.seed(123)

enet_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = seq(0, 1, by = 0.1)
)

model_enet <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = enet_grid
)

models[["enet"]] <- model_enet
```

```{r model-diagnostic-1, echo=FALSE}
pred <- predict(model_reg1, data_train)
pred <- data.frame(predicted_value = pred, observed_value = data_train$logTotalValue)
pl_model1_reg1 <- ggplot(data = pred, aes(x = observed_value, y = predicted_value)) +
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point(alpha = 0.1)

pred <- predict(model_reg2, data_train)
pred <- data.frame(predicted_value = pred, observed_value = data_train$logTotalValue)
pl_model1_reg2 <- ggplot(data = pred, aes(x = observed_value, y = predicted_value)) +
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point(alpha = 0.1)

pl_model1_lasso <- plot(model_lasso)
pl_model1_ridge <- plot(model_ridge)
pl_model1_enet <- plot(model_enet)
```


### Compare models - on CV RMSE

We can see that the model with the lowest RMSE is the regression with only the 10 best predictors, and the worst is the extended multiple regression model with all the variables. We can see that penalizing the models definitely improves the model performance, and that Elastic Net is the best in this case with parameters of $\alpha =$ `r model_enet$bestTune$alpha` and $\lambda =$ `r model_enet$bestTune$lambda`

```{r model-comparison-1}
rmses <- c()

for (model in models) {
  rmses <- c(rmses, mean(model$resample$RMSE))
}

results <- data.frame(`Model Name` = names(models), RMSE = rmses) %>% arrange(RMSE)

results_list[["lowest_RMSE"]] <- results
```


```{r model-comparison-1-table, results='asis', echo=FALSE}
kable(results, format = "html", table.attr = "style='width:60%;'") %>% 
  kableExtra::kable_styling()

```

## Model building 2.

Now, we are going to use a different selection function when we pick the best model. Instead of choosing the model with the lowest CV RMSE, it will pick the worst-best model. That is, the simplest model that still is within one SE of the model with the lowest RMSE. This will result in a slightly worse performance, however it might improve the interpretability of our model. Note, that this will only affect our regularized models, that have tuning parameters (LASSO, Ridge, and Elastic net), as OLS will always give the same result.

```{r train-control-2}
# set crossvalidation
trctrl <- trainControl(
  method = "cv",
  number = 10,
  selectionFunction = "oneSE"
)

# init list of models
models <- list()

```

```{r model-building-2}
# Regression with only the best predictors
set.seed(123)
model_reg1 <- train(
  formula_reg1,
  data = data_train,
  method = "lm",
  trControl = trctrl
)

models[["reg_simple"]] <- model_reg1

# Extended regression
set.seed(123)
model_reg2 <- train(
  formula_reg2,
  data = data_train,
  method = "lm",
  trControl = trctrl
)

models[["reg_extended"]] <- model_reg2

# LASSO
set.seed(123)

lasso_grid <- expand.grid(
  "alpha" = 1,
  "lambda" = seq(0, 1, by = 0.1)
)

model_lasso <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = lasso_grid
)

models[["lasso"]] <- model_lasso

# Ridge
set.seed(123)

ridge_grid <- expand.grid(
  "alpha" = 0,
  "lambda" = seq(0, 1, by = 0.1)
)

model_ridge <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = ridge_grid
)

models[["ridge"]] <- model_ridge

# Elastic Net
set.seed(123)

enet_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = seq(0, 1, by = 0.1)
)

model_enet <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = enet_grid
)

models[["enet"]] <- model_enet

```

```{r model-diagnostic-2, echo=FALSE}
pred <- predict(model_reg1, data_train)
pred <- data.frame(predicted_value = pred, observed_value = data_train$logTotalValue)
pl_model2_reg1 <- ggplot(data = pred, aes(x = observed_value, y = predicted_value)) +
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point(alpha = 0.1)

pred <- predict(model_reg2, data_train)
pred <- data.frame(predicted_value = pred, observed_value = data_train$logTotalValue)
pl_model2_reg2 <- ggplot(data = pred, aes(x = observed_value, y = predicted_value)) +
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point(alpha = 0.1)

pl_model2_lasso <- plot(model_lasso)
pl_model2_ridge <- plot(model_ridge)
pl_model2_enet <- plot(model_enet)
```

### Compare models - on CV RMSE

In this run, caret selected the best model based on the "oneSE" rule. That means, that it will choose the simplest model, that is within one SE of the best model (in terms of RMSE). What we can see now, is that Elastic net "became" ridge, their $\alpha$ and $\lambda$ parameters are the same. We can see that in some cases these are lower (e.g. elastic net) and in some cases they stayed the same (e.g. ridge). Also, in the case of LASSO, the performance decreased a bit.

```{r model-comparison-2}
rmses <- c()

for (model in models) {
  rmses <- c(rmses, mean(model$resample$RMSE))
}

results <- data.frame(`Model Name` = names(models), RMSE = rmses) %>% arrange(RMSE)

results_list[["oneSE_RMSE"]] <- results

compare_enet_ridge <- data.frame(
  `Model Name` = c("enet", "ridge"),
  alpha = c(model_enet$bestTune$alpha, model_ridge$bestTune$alpha),
  lambda = c(model_enet$bestTune$lambda, model_ridge$bestTune$lambda)
)
```

```{r model-comparison-2-table, results='asis', echo=FALSE}
kable(results, col.names = c("Model Name", "RMSE"), format = "html", table.attr = "style='width:60%;'") %>% 
  kableExtra::kable_styling()

kable(compare_enet_ridge, col.names = c("Model Name", "Alpha", "Lambda"), format = "html", table.attr = "style='width:60%;'") %>% 
  kableExtra::kable_styling()

```

## PCA

PCA, or Principal Component Analysis is a non parametric dimensionality reduction method that allows us to simplify our features in a way that retains the most information (variance). This is very useful if we have a lot of features and few observations, or we have a lot of features and lot of observations and a complex machine learning model. In the latter case this will help us reduce the time it takes to train our model.

PCA does this by creating new "features" (or components) by rotating the matrix that represents our dataset. It does this in a way to maximize the sum of squared distances from the center. The rotation that achieves this can be characterized by a vector (that is called the eigenvector), and the vector that can capture the most variance (described by the eigenvalue) will be the first component. It then finds another vector by rotating the matrix orthogonally to the first vector and again maximizing the sum of squared distances. It does this for all components. At the end there will be a new matrix of features x components, and each value will determine the load of a feature to a component, that is, how important each variable is in a component (how much variance is captured by that component from that variable). Components are ordered by the share of variance explained, so the first principal component explains the most, the second the second most, etc.

In the plot below, we can see the share of variance for each component, and the cumulative variance explained by the components. Based on this, there are essentially two points where it is reasonable to choose a component number to use: 50, where the scree plot curve breaks first, or perhaps where the cumulative variance explained is around 80 percent. I tried out both, and based on the performance of these models, I choose the number of components that explained 80 percent of the total variation.

```{r one-hot}
data_one_hot <- data.table(data) %>% dplyr::select(where(is.factor)) %>% one_hot(dropUnusedLevels = T) 
is_not_fac <- function(x) !is.factor(x)

data_pca <- cbind(data, data_one_hot) %>% dplyr::select(where(is_not_fac))
```


```{r pca_results, include=FALSE}
pca_result <- prcomp(data_pca, scale. = T, center = T)
print(pca_result)

variances <- pca_result$sdev^2
total_variance <- sum(variances)
total_variance
```

```{r, include=FALSE}
share_variance_by_component <- variances / total_variance
dt_variance <- data.table(component = 1:length(variances),
                          share_variance = share_variance_by_component)
dt_variance[, cum_share_variance := cumsum(share_variance)]
```

```{r}
ggplot(data = melt(dt_variance, id.vars = "component")) +
  geom_line(aes(x = component, y = value, color = variable)) +
  facet_wrap(~ variable, scales = "free_y") +
  theme(legend.position = "bottom")
```

```{r, include=FALSE}
pre_process <- preProcess(data_pca, method = c("center", "scale", "pca"), thresh = 0.80)
pre_process
```

This PCA needed `r pre_process$numComp` variables to capture `r pre_process$thresh*100` percent of the variance.

## Model building 3.

In this final round I am going to fit the previous models on the transformed dataset and will use PCA to extract the first $n$ factors that explain 80 percent of the total variation.

```{r train-control-3}
# set crossvalidation
trctrl <- trainControl(
  method = "cv",
  number = 10,
  preProcOptions = list(thresh = 0.80)
)

set.seed(1234)
training_ratio <- 0.3
train_indices <- createDataPartition(
  y = data_pca[["logTotalValue"]],
  times = 1,
  p = training_ratio,
  list = FALSE
) %>% as.vector()
data_train <- data_pca[train_indices, ]
data_test <- data_pca[-train_indices, ]

# init list of models
models <- list()

```

```{r model-building-3}
# Regression with only the best predictors
set.seed(123)
model_reg1 <- train(
  formula_reg1,
  data = data_train,
  method = "lm",
  trControl = trctrl,
  preProcess = c("center", "scale", "pca", "nzv")
)

models[["reg_simple"]] <- model_reg1

# Extended regression
set.seed(123)
model_reg2 <- train(
  formula_reg2,
  data = data_train,
  method = "lm",
  trControl = trctrl,
  preProcess = c("center", "scale", "pca", "nzv")
)

models[["reg_extended"]] <- model_reg2

# LASSO
set.seed(123)

lasso_grid <- expand.grid(
  "alpha" = 1,
  "lambda" = seq(0, 1, by = 0.1)
)

model_lasso <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = lasso_grid,
  preProcess = c("center", "scale", "pca", "nzv")
)

models[["lasso"]] <- model_lasso


# Ridge
set.seed(123)

ridge_grid <- expand.grid(
  "alpha" = 0,
  "lambda" = seq(0, 1, by = 0.1)
)

model_ridge <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = ridge_grid,
  preProcess = c("center", "scale", "pca", "nzv")
)

models[["ridge"]] <- model_ridge

# Elastic Net
set.seed(123)

enet_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = seq(0, 1, by = 0.1)
)

model_enet <- train(
  formula_reg2,
  data = data_train,
  method = "glmnet",
  trControl = trctrl,
  tuneGrid = enet_grid,
  preProcess = c("center", "scale", "pca", "nzv")
)

models[["enet"]] <- model_enet

```

```{r model-diagnostic-3, echo=FALSE}
pred <- predict(model_reg1, data_train)
pred <- data.frame(predicted_value = pred, observed_value = data_train$logTotalValue)
pl_model3_reg1 <- ggplot(data = pred, aes(x = observed_value, y = predicted_value)) +
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point(alpha = 0.1)

pred <- predict(model_reg2, data_train)
pred <- data.frame(predicted_value = pred, observed_value = data_train$logTotalValue)
pl_model3_reg2 <- ggplot(data = pred, aes(x = observed_value, y = predicted_value)) +
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_point(alpha = 0.1)

pl_model3_lasso <- plot(model_lasso)
pl_model3_ridge <- plot(model_ridge)
pl_model3_enet <- plot(model_enet)
```

### Compare models - on CV RMSE

Here we can see the performance of our models when they were trained on components generated by PCA instead of the original features. We can see that all of our model's performance increased compared to previous build's, except for our simple regression. This is because the features that were included might not be covered by the 10 predictors which we used before.

```{r model-comparison-3}
rmses <- c()

for (model in models) {
  rmses <- c(rmses, mean(model$resample$RMSE))
}

results <- data.frame(`Model Name` = names(models), RMSE = rmses) %>% arrange(RMSE)

results_list[["pca_RMSE"]] <- results
```

```{r model-comparison-3-table, results='asis', echo=FALSE}
kable(results, col.names = c("Model Name", "RMSE"), format = "html", table.attr = "style='width:60%;'") %>% 
  kableExtra::kable_styling()
```


## Final comparison between models

So to sum it up, PCA increased all of our models' predictive performance, except for the simple regression, which it made considerably worse. The best model(s) were Elastic net/Ridge when using PCA with an 80 percent threshold.

```{r final-comp}
# order results in model alphabetical order
for (resind in 1:length(results_list)) {
  results_list[[resind]]$Model.Name <- as.character(results_list[[resind]]$Model.Name)
  results_list[[resind]] <- results_list[[resind]] %>% arrange(Model.Name)
}


lowest_RMSE <- results_list$lowest_RMSE$RMSE
oneSE_RMSE <- results_list$oneSE_RMSE$RMSE
pca_RMSE <- results_list$pca_RMSE$RMSE

final_results <- data.frame(
  model_name = results_list$lowest_RMSE$Model.Name,
  lowest_rmse = lowest_RMSE,
  onse_rmse = oneSE_RMSE,
  pca_rmse = pca_RMSE
)
```

```{r final-comp-table, results='asis'}
kable(final_results, col.names = c("Model Name", "RMSE - lowest", "RMSE - oneSE", "RMSE - PCA"), format = "html", table.attr = "style='width:60%;'") %>% 
  kableExtra::kable_styling()

```

## Evaluation on test set

```{r evaluate-test}
enet_pred <- predict(model_enet, newdata = data_test)

enet_RMSE <- RMSE(enet_pred, data_test$logTotalValue)
```

The RMSE on the test set for the elastic net model is `r enet_RMSE`.

# Clustering on the USArrests datasets

```{r}
library(NbClust)
library(factoextra)
```


```{r}
data <- USArrests

skim(data)
```

We need to scale the data, and centering also can not hurt us when we are clustering. Scaling is necessary, because K-means clustering uses distances as measures, and if one variable has a greater scale, it will have a bigger impact on clustering.

```{r}
data_cent <- data %>% 
  mutate_all(scale)
```

To determine the optimal number of clusters we can use "majority voting" based on different indexes. The number of clusters in this case is 2.

```{r message=FALSE, warning=FALSE, include=FALSE}
arrests <- NbClust(data_cent, method = "kmeans", min.nc = 2, max.nc = 10)
```


```{r include=FALSE, message=FALSE, warning=FALSE}
pl_nbclust <- fviz_nbclust(arrests)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
pl_nbclust
```

Now we are prepared to cluster our data using kmeans clustering. We can see on the plots below, how groups are assigned. On the first plot we see Urban Population against Assaults. On the second and third we see the groups projected onto the first and second PC. 

```{r}
set.seed(123)
km <- kmeans(data_cent, centers = 2)
group <- factor(km$cluster)
state <- names(km$cluster)
tdf <- data.frame(
  group = group,
  state = as.character(state)
)

data_grouped <- data
data_grouped$state <- row.names(data)
row.names(data_grouped) <- NULL
row.names(tdf) <- NULL

# tdf$state <- as.character(tdf$state)
data_grouped$state <- as.factor(data_grouped$state)

data_grouped <- data_grouped %>% 
  left_join(tdf, by = "state")
```

```{r message=FALSE, warning=FALSE}
ggplot(data_grouped, aes(UrbanPop, Assault, color = group, shape = group)) + 
  geom_point(size = 2) + 
  theme_bw() + 
  scale_x_continuous(limits = c(0, 100)) + 
  scale_y_continuous(limits = c(0, 400))
```

```{r}
pca_result <- prcomp(data, scale = TRUE)
first_two_pc <- as_tibble(pca_result$x[, 1:2])
first_two_pc$group <- data_grouped$group

ggplot(first_two_pc, aes(PC1, PC2, color = group, shape = group)) + 
  geom_point(size = 2) + 
  scale_x_continuous(limits = c(-3, 3), breaks = seq(-3, 3, by = 0.5)) + 
  scale_y_continuous(limits = c(-3, 3), breaks = seq(-3, 3, by = 0.5)) + 
  theme_bw()

fviz_pca(pca_result)
```

We can also check what each component "means" by investigating the factor loadings. PC1 correlates with the different offences more, while the second component captures how urbanized a state is.

```{r}
strength <- pca_result$rotation[, 1:2]

featuresDF <- data.frame(
  features = dimnames(strength)[[1]],
  PC1 = strength[, 1],
  PC2 = strength[, 2]
)
rownames(featuresDF) <- NULL

featuresDF <- featuresDF %>% pivot_longer(c(PC1, PC2), names_to = "component", "values_to" = "loading")

ggplot(featuresDF, aes(features, abs(loading))) + 
  geom_bar(stat="identity", fill = "white", color = "black") +
  facet_grid(cols = vars(component)) + 
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25)) + 
  theme_bw()
```

# PCA of high dimensional data

```{r}
data <- read_csv("https://www.statlearning.com/s/Ch10Ex11.csv", col_names = FALSE) %>%
  t() %>% as_tibble()  # the original dataset is of dimension 1000x40 so we transpose it
# dim(data)
```

```{r}
pca_result <- prcomp(data, scale. = T)
```

Let's see the data projected on the first two PC. We can see the two groups (healthy on the left and not healthy on the right) clearly separated.

```{r}
fviz_pca_ind(pca_result)
```

```{r}
strength <- pca_result$rotation

featuresDF <- data.frame(
  features = dimnames(strength)[[1]],
  PC1 = strength[, 1],
  PC2 = strength[, 2]
)
rownames(featuresDF) <- NULL

pc1_feature <- featuresDF %>% mutate(PC1 = abs(PC1)) %>% arrange(desc(PC1)) %>% top_n(1, wt = PC1)
pc2_feature <- featuresDF %>% mutate(PC2 = abs(PC2)) %>% arrange(desc(PC2)) %>% top_n(1, wt = PC2)

pc1_x <- as.character(pc1_feature[1,]$features)
pc2_y <- as.character(pc2_feature[1,]$features)

data$healthy <- as.factor(c(rep(1, 20), rep(0, 20)))
```

We can also take a look at the the data visualised in the coordinate system of the highest loading features of the first and second PC. It is very similar to the plot before, but not as clearly separated. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
data %>% 
  ggplot(aes(V502, V989, color = healthy, shape = healthy)) +
  geom_point(size = 2) + 
  scale_y_continuous(limits = c(-4, 4), breaks = seq(-4, 4, by = 1)) + 
  scale_x_continuous(limits = c(-4, 4), breaks = seq(-4, 4, by = 1)) + 
  theme_bw() 

```



# Appendix

## Part 1.

### Exploratory data analysis plots

```{r eda-plots, echo=FALSE, message=FALSE, warning=FALSE}
# look at histograms
pl_hist_totalvalue

pl_hist_logtotalvalue

# check boxplots
pl_box_zonedist 
pl_box_ownertype
pl_box_landmark
pl_box_historicdist
pl_box_schooldist 
pl_box_built 
pl_box_council 
pl_box_police 

# check scatterplots
pl_scatter_comarea 
pl_scatter_facilfar
```

### Model building 1. - best by lowest RMSE diagnostic plots

```{r model-1-plots, cache=FALSE}
pl_model1_reg1
pl_model1_reg2
pl_model1_lasso
pl_model1_ridge
pl_model1_enet
```

### Model buildign 2. - best by oneSE diagnostic plots

```{r model-2-plots, cache=FALSE}
pl_model2_reg1
pl_model2_reg2
pl_model2_lasso
pl_model2_ridge
pl_model2_enet
```

### Model building 3. - models after PCA diagnostic plots

```{r model-3-plots, cache=FALSE}
pl_model3_reg1
pl_model3_reg2
pl_model3_lasso
pl_model3_ridge
pl_model3_enet
```

