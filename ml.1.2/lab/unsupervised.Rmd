---
title: "Lab week 2 - Unsupervised methods"
subtitle: "Data Science 2: Machine Learning Concepts - CEU 2021"
author: "Janos K. Divenyi, Jeno Pal"
date: '2021-02-15'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: inline
---

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(skimr)
library(janitor)

library(factoextra) # provides nice functions for visualizing the output of PCA
library(NbClust) # for choosing the optimal number of clusters

library(knitr)
library(kableExtra)

theme_set(theme_minimal())
```

Main reference for these topics: [Introduction to Statistical
Learning](http://www-bcf.usc.edu/~gareth/ISL/), Chapters 6 and 10.
Excellent, simple explanations on deep issues.

StatQuest videos you might find helpful: - [PCA
\#1](https://www.youtube.com/watch?v=HMOI_lkzW08&ab_channel=StatQuestwithJoshStarmer)
- [PCA
\#2](https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer)
- [K-Means
clustering](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)
- [Hierarchical
clustering](https://www.youtube.com/watch?v=7xHsRkOdVwo&ab_channel=StatQuestwithJoshStarmer)

## Principal Component Analysis (PCA)

We transform the coordinates of the original variables to capture as
much variation as we can with independent (orthogonal) dimensions. For a
very nice illustration and discussion, see
[here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).

```{r}
data <- USArrests
print(skim(data))
```

## An example with two variables

Take two variables first only. We seek a linear combination of them that
*has the highest possible variance* with weights that are normalized
(their sum of squares are 1)

```{r}
df_murder_assault <- select(data, Murder, Assault)
ggplot(df_murder_assault, aes(Murder, Assault)) +
  geom_point()
```

```{r}
# the variables have totally different variances
summarise(df_murder_assault, across(everything(), var))
```

Demean the data only for easier visualizations.

```{r}
df_murder_assault_demeaned <- mutate(df_murder_assault, across(everything(), ~ .x - mean(.x)))
```

```{r}
ggplot(df_murder_assault_demeaned, aes(Murder, Assault)) +
  geom_point()
```

The goal is to find a linear combination of the two variables that
captures most of the joint variance. Indeed, we see that we get back the
weight we obtained from the `prcomp` function.

```{r}
# constraint: w_assault^2 + w_murder^2 = 1
# this means that w_murder = sqrt(1 - w_assault^2)

objective <- function(w_assault) {
  # we want to maximize variance
  # minus: since "optim" applies minimization.
  -var(
    w_assault * df_murder_assault_demeaned$Assault +
    sqrt(1 - w_assault^2) * df_murder_assault_demeaned$Murder
  )
}

optim_result <- optimize(f = objective, interval = c(0, 1), tol = 1e-15)
w_assault <- optim_result$minimum
w_murder <- sqrt(1 - w_assault^2)
message(glue::glue("Weight of assault: {round(w_assault, 7)} \n Weight of murder: {round(w_murder, 7)}"))
```

With PCA we can arrive at the same result.

```{r}
# Note from the help of prcomp:
# "The signs of the columns of the rotation matrix are arbitrary, and so may differ between different programs for PCA, and even between different builds of R."
pca_murder_assault <- prcomp(df_murder_assault_demeaned)
pca_murder_assault
```

```{r}
pc1 <- pca_murder_assault$rotation[, "PC1"]
pc1
```

Let us depict this variance-maximizing linear combination of the two
variables in the space of the original variables.

```{r}
ggplot(df_murder_assault_demeaned, aes(Murder, Assault)) +
  geom_point() +
  geom_abline(slope = pc1[["Assault"]] / pc1[["Murder"]], color = "red")
```

WARNING: this line is very different from regressing Assault on Murder!
PCA's aim is to find a line to which if observations are projected,
variance is the highest. Regression: squared errors to be minimized.

```{r}
ggplot(df_murder_assault_demeaned, aes(Murder, Assault)) +
  geom_point() +
  geom_abline(slope = pc1[["Assault"]] / pc1[["Murder"]], color = "red") +
  geom_abline(slope = coef(lm(Assault ~ Murder, data = df_murder_assault_demeaned))[["Murder"]], color = "blue")
```

See more about it
[here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579).

## The full example

Scaling: necessary to have comparable units of different variables.
(Multiplying a variable by 100 increases the variance and PCA would try
to capture that, even though it is only an artifact of units. We don't
want that.) In the previous example we did not scale all variables for
the sake of nicer illustrations and we indeed saw that the variable with
the higher absolute variance got a lot more weight than the other. This
is not really what we are after.

Let's perform PCA on all 4 variables. The first principal component
shows weights for a linear combination of the original variables that
maximizes variance (up to constraining weights to have a sum of square
equal to 1).

```{r}
pca_result <- prcomp(data, scale. = TRUE)
print(pca_result)
```

```{r}
names(pca_result)
```

```{r}
# weights indeed sum to one
colSums(pca_result$rotation^2)
```

```{r}
# PCA components are orthogonal to each other: they contain "independent" variance from the data
sum(pca_result$rotation[, 1] * pca_result$rotation[, 2])
```

We can plot how much of total variance is captured by subsequent
principal components (and in total).

```{r}
variances <- pca_result$sdev^2
variances
```

Indeed, if we take the linear combination of (scaled) original variables
with the weights specified by PC1, we should get back the value of the
first element of `variances`.

```{r}
pc1_loadings <- pca_result$rotation[, "PC1"]
pc1_value_for_observations <- scale(data) %*% pc1_loadings  # %*%: matrix-vector product
var(pc1_value_for_observations)
```

Total variance: if scaling was done, it equals the number of variables
(since it is 1 for each variable).

```{r}
total_variance <- sum(variances)
total_variance
```

We can inspect how large share of the variance is captured by certain
number of principal components.

```{r}
share_variance_by_component <- variances / total_variance
df_variance <- tibble(
  component = 1:length(variances),
  share_variance = share_variance_by_component
) %>%
  mutate(cum_share_variance = cumsum(share_variance))
```

```{r}
ggplot(data = pivot_longer(df_variance, -component)) +
  geom_line(aes(x = component, y = value, color = name)) +
  facet_wrap(~ name) +
  theme(legend.position = "bottom")
```

How many components summarize the data? No definite answer: decide based
on sufficient variance explained.

We can plot the relative contribution of original variables to principal
components. They are just a visual display of the relative weights that
these variables represent in creating the principal components.

```{r}
fviz_contrib(pca_result, "var", axes = 1)
```

```{r}
fviz_contrib(pca_result, "var", axes = 2)
```

We can plot observations as well as original features in the space
spanned by the first two principal components.

```{r}
fviz_pca(pca_result)
```

### PCA with `caret`

We can use `preProcess` from `caret` to perform the same
transformations. These can serve as inputs to `train`.

```{r}
pre_process <- preProcess(data, method = c("center", "scale", "pca"))
pre_process
```

```{r}
pre_process$rotation
```

```{r}
pre_process <- preProcess(data, method = c("center", "scale", "pca"), pcaComp = 4)
pre_process$rotation
```

```{r}
preProcess(data, method = c("center", "scale", "pca"), thresh = 0.8)
```

### Using PCA as an input to supervised learning

Let's predict baseball player salaries using PCA.

```{r}
data <- ISLR::Hitters
print(skim(data))
```

```{r}
data <- drop_na(data, Salary)
```

Train a simple linear model:

```{r}
set.seed(857)
lm_fit <- train(
  Salary ~ . ,
  data = data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("center", "scale")
)
lm_fit
```

We can use PCA with specified number of components (or we can also set
the `thresh` argument to set a threshold that PCA components explain at
least a certain share of the variance):

```{r}
set.seed(857)
lm_fit_pca <- train(
  Salary ~ . ,
  data = data,
  method = "lm",
  trControl = trainControl(
    method = "cv",
    number = 10,
    preProcOptions = list(pcaComp = 18)),
  preProcess = c("center", "scale", "pca")
)
lm_fit_pca
lm_fit_pca$preProcess
```

Method `pcr` implements precisely this: linear regression with principal
components as explanatory variables. Its hyperparameter is the number of
components to be used.

```{r}
tune_grid <- data.frame(ncomp = 1:19)
set.seed(857)
pcr_fit <- train(
  Salary ~ . ,
  data = data,
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = tune_grid,
  preProcess = c("center", "scale")
)
pcr_fit
```

## K-means clustering

Goal: group observations that are more similar to each other than to
others.

```{r}
data <- clean_names(iris)
print(skim(data))
```

```{r}
ggplot(data, aes(x = petal_length, y = sepal_length, color = species)) + geom_point()
```

Suppose we do not know labels but want to group observations based on
features.

```{r}
data_features <- select(data, -species)
km <- kmeans(data_features, centers = 3)
km
```

```{r}
data_w_clusters <- mutate(data_features, cluster = factor(km$cluster))

ggplot(data_w_clusters, aes(x = petal_length, y = sepal_length, color = cluster)) +
  geom_point()
```

We can inspect the resulting centers.

```{r}
km$centers
```

```{r}
centers <- as_tibble(km$centers) %>% 
  mutate(cluster = factor(seq(nrow(km$centers))), center = TRUE)
kable(centers, digits = 3) %>%
  kable_styling(full_width = F)
```

```{r}
plot_clusters_with_centers <- function(features, kmeans_object) {

  data_w_clusters <- mutate(features, cluster = factor(kmeans_object$cluster))

  centers <- as_tibble(kmeans_object$centers) %>%
    mutate(cluster = factor(seq(nrow(km$centers))), center = TRUE)

  data_w_clusters_centers <- bind_rows(data_w_clusters, centers)
  ggplot(data_w_clusters_centers, aes(
    x = petal_length, y = sepal_length,
    color = cluster, size = ifelse(!is.na(center), 2, 1))
  ) +
    geom_point() +
    scale_size(guide = 'none')
}

plot_clusters_with_centers(data_features, km)
```

Results depend on the starting centers which are randomly chosen
observations.

```{r}
set.seed(1122)
km <- kmeans(data_features, centers = 3, nstart = 1)
print(km$centers)
print(table(km$cluster))
print(km$withinss)

plot_clusters_with_centers(data_features, km)
```

```{r}
set.seed(223456)
km <- kmeans(data_features, centers = 3, nstart = 1)
print(km$centers)
print(table(km$cluster))
print(km$withinss)

plot_clusters_with_centers(data_features, km)
```

We should always experiment with different starting values (probably
generated randomly). `nstart` controls how many times the algorithm is
run with different random starting points for the centers. Setting it to
a high value (e.g., 20) is a good idea to achieve the best groupings.

### Choosing K

There are no general rules, depends on the application. There are some
rules of thumb, though. There are many, you can explore some with
explanation, [here](http://www.sthda.com/english/wiki/print.php?id=239).

A popular one looks at the evolution of the within-sum-of-squares, and
identifies the "elbow" point for the optimal number of clusters

```{r}
fviz_nbclust(data_features, kmeans, method = "wss")
```

`NbClust` calculates 30 indices based on various principles and chooses
by majority rule.

```{r, results="hide"}
nb <- NbClust(data_features, method = "kmeans", min.nc = 2, max.nc = 10, index = "all")
nb
```

## Hierarchical clustering

With hierarchical clustering we get a nested structure of clusters based
on a dissimilarity measure. Is it better than k-means? It depends -
k-means does not yield a hierarchical structure. If the data does not
have one in reality, hierarchical may be not as good as k-means and
there are reversed situations as well.

```{r}
data_distances <- dist(data_features)
# use the average distance between groups to decide which
# groups to merge next
hc <- hclust(data_distances, method = "average")
```

```{r}
fviz_dend(hc)
```

```{r}
fviz_dend(hc, k = 3)
```

```{r}
fviz_dend(hc, k = 6)
```

```{r}
# get labels
cluster_hc <- cutree(hc, 3)
data_w_clusters <- mutate(data_features, cluster = factor(cluster_hc))

ggplot(data_w_clusters, aes(x = petal_length, y = sepal_length, color = cluster)) +
  geom_point()
```

How we calculate similarities between groups may strongly affect the
clustering.

```{r}
# max distance between points of two groups
hc_complete <- hclust(data_distances, method = "complete")
fviz_dend(hc_complete, k = 3)
```

```{r}
# single: minimal distance between points of two groups
hc_single <- hclust(data_distances, method = "single")
fviz_dend(hc_single, k = 3)
```

## General considerations on clustering

-   as both clustering methods are based on distances in features, we
    may want to first bring all variables to the same scale
-   clusters may be sensitive to details such as scaling or not, whether
    we take subsets of data or what is the dissimilarity measure in
    hierarchical clustering. Advice: experiment with the settings and
    look for consistent patterns.
