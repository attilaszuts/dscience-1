---
title: "Lab week 2 - Unsupervised methods"
subtitle: "Data Science 2: Machine Learning Concepts - CEU 2021"
author: "Janos K. Divenyi, Jeno Pal"
date: '2021-02-15'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(skimr)
library(janitor)

library(factoextra) # provides nice functions for visualizing the output of PCA
library(NbClust) # for choosing the optimal number of clusters

library(knitr)
library(kableExtra)

theme_set(theme_minimal())
```

Main reference for these topics: [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), Chapters 6 and 10. Excellent, simple explanations on deep issues.

StatQuest videos you might find helpful:
  - [PCA #1](https://www.youtube.com/watch?v=HMOI_lkzW08&ab_channel=StatQuestwithJoshStarmer)
  - [PCA #2](https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer)
  - [K-Means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)
  - [Hierarchical clustering](https://www.youtube.com/watch?v=7xHsRkOdVwo&ab_channel=StatQuestwithJoshStarmer)


## Principal Component Analysis (PCA)

We transform the coordinates of the original variables to capture as much
variation as we can with independent (orthogonal) dimensions.
For a very nice illustration and discussion, see [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).

```{r}
data <- USArrests
print(skim(data))
```

## An example with two variables

Take two variables first only. We seek a linear combination of them that
* has the highest possible variance
* with weights that are normalized (their sum of squares are 1)

```{r}
df_murder_assault <- select(data, Murder, Assault)
ggplot(df_murder_assault, aes(Murder, Assault)) +
  geom_point()
```

```{r}
# the variables have totally different variances
summarise(df_murder_assault, across(everything(), var))
```

Demean the data only for easier visualizations.
```{r}
df_murder_assault_demeaned <- mutate(df_murder_assault, across(everything(), ~ .x - mean(.x)))
```

```{r}
ggplot(df_murder_assault_demeaned, aes(Murder, Assault)) +
  geom_point()
```

The goal is to find a linear combination of the two variables that captures most of the joint
variance. Indeed, we see that we get back the weight we obtained from the `prcomp` function.

```{r}
# constraint: w_assault^2 + w_murder^2 = 1
# this means that w_murder = sqrt(1 - w_assault^2)

objective <- function(w_assault) {
  # we want to maximize variance
  # minus: since "optim" applies minimization.
  -var(
    w_assault * df_murder_assault_demeaned$Assault +
    sqrt(1 - w_assault^2) * df_murder_assault_demeaned$Murder
  )
}

optim_result <- optimize(f = objective, interval = c(0, 1), tol = 1e-15)
w_assault <- optim_result$minimum
w_murder <- sqrt(1 - w_assault^2)
message(glue::glue("Weight of assault: {round(w_assault, 7)} \n Weight of murder: {round(w_murder, 7)}"))
```

With PCA we can arrive at the same result.

```{r}
# Note from the help of prcomp:
# "The signs of the columns of the rotation matrix are arbitrary, and so may differ between different programs for PCA, and even between different builds of R."
pca_murder_assault <- prcomp(df_murder_assault_demeaned)
pca_murder_assault
```

```{r}
pc1 <- pca_murder_assault$rotation[, "PC1"]
pc1
```

Let us depict this variance-maximizing linear combination of the two variables
in the space of the original variables.
```{r}
ggplot(df_murder_assault_demeaned, aes(Murder, Assault)) +
  geom_point() +
  geom_abline(slope = pc1[["Assault"]] / pc1[["Murder"]], color = "red")
```

WARNING: this line is very different from regressing Assault on Murder! PCA's aim is to find a line to which if
observations are projected, variance is the highest. Regression: squared errors to be minimized.

```{r}
ggplot(df_murder_assault_demeaned, aes(Murder, Assault)) +
  geom_point() +
  geom_abline(slope = pc1[["Assault"]] / pc1[["Murder"]], color = "red") +
  geom_abline(slope = coef(lm(Assault ~ Murder, data = df_murder_assault_demeaned))[["Murder"]], color = "blue")
```

See more about it [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579).

## The full example

Scaling: necessary to have comparable units of different variables.
(Multiplying a variable by 100 increases the variance and PCA would try to
capture that, even though it is only an artifact of units. We don't want that.)
In the previous example we did not scale all variables for the sake of nicer illustrations and we indeed saw that the variable with the higher absolute variance got a lot more weight than the other. This is not really what we are after.

Let's perform PCA on all 4 variables. The first principal component shows weights for a linear combination of the original variables that maximizes variance (up to constraining weights to have a sum of square equal to 1).
```{r}
pca_result <- prcomp(data, scale. = TRUE)
print(pca_result)
```

```{r}
names(pca_result)
```

```{r}
# weights indeed sum to one
colSums(pca_result$rotation^2)
```

```{r}
# PCA components are orthogonal to each other: they contain "independent" variance from the data
sum(pca_result$rotation[, 1] * pca_result$rotation[, 2])
```

We can plot how much of total variance is captured by subsequent principal
components (and in total).
```{r}
variances <- pca_result$sdev^2
variances
```

Indeed, if we take the linear combination of (scaled) original variables with the weights specified by
PC1, we should get back the value of the first element of `variances`.

```{r}
pc1_loadings <- pca_result$rotation[, "PC1"]
pc1_value_for_observations <- scale(data) %*% pc1_loadings  # %*%: matrix-vector product
var(pc1_value_for_observations)
```

Total variance: if scaling was done, it equals
the number of variables (since it is 1 for each variable).
```{r}
total_variance <- sum(variances)
total_variance
```

We can inspect how large share of the variance is captured by certain number of principal components.
```{r}
share_variance_by_component <- variances / total_variance
df_variance <- tibble(
  component = 1:length(variances),
  share_variance = share_variance_by_component
) %>%
  mutate(cum_share_variance = cumsum(share_variance))
```

```{r}
ggplot(data = pivot_longer(df_variance, -component)) +
  geom_line(aes(x = component, y = value, color = name)) +
  facet_wrap(~ name, scales = "free_y") +
  theme(legend.position = "bottom")
```

How many components summarize the data? No definite answer: decide based
on sufficient variance explained.

We can plot the relative contribution of original variables to principal components. They are just a
visual display of the relative weights that these variables represent in creating the principal components. 
```{r}
fviz_contrib(pca_result, "var", axes = 1)
```
```{r}
fviz_contrib(pca_result, "var", axes = 2)
```

We can plot observations as well as original features in the space spanned
by the first two principal components.
```{r}
fviz_pca(pca_result)
```

### PCA with `caret`

We can use `preProcess` from `caret` to perform the same transformations. These
can serve as inputs to `train`.
```{r}
pre_process <- preProcess(data, method = c("center", "scale", "pca"))
pre_process
```
```{r}
pre_process$rotation
```

```{r}
pre_process <- preProcess(data, method = c("center", "scale", "pca"), pcaComp = 4)
pre_process$rotation
```

```{r}
preProcess(data, method = c("center", "scale", "pca"), thresh = 0.999)
```

### Using PCA as an input to supervised learning

Let's predict baseball player salaries using PCA.
```{r}
data <- ISLR::Hitters
print(skim(data))
```
```{r}
data <- drop_na(data, Salary)
```

Train a simple linear model:
```{r}
set.seed(857)
lm_fit <- train(
  Salary ~ . ,
  data = data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("center", "scale")
)
lm_fit
```

We can use PCA with specified number of components (or we can also
set the `thresh` argument to set a threshold that PCA components
explain at least a certain share of the variance):
```{r}
set.seed(857)
lm_fit_pca <- train(
  Salary ~ . ,
  data = data,
  method = "lm",
  trControl = trainControl(
    method = "cv",
    number = 10,
    preProcOptions = list(pcaComp = 18)),
  preProcess = c("center", "scale", "pca")
)
lm_fit_pca
lm_fit_pca$preProcess
```

Method `pcr` implements precisely this: linear regression with principal
components as explanatory variables. Its hyperparameter is the number of
components to be used.
```{r}
tune_grid <- data.frame(ncomp = 1:19)
set.seed(857)
pcr_fit <- train(
  Salary ~ . ,
  data = data,
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = tune_grid,
  preProcess = c("center", "scale")
)
pcr_fit
```

## K-means clustering

Goal: group observations that are more similar to each other than to others.

```{r}
data <- clean_names(iris)
print(skim(data))
```

```{r}
ggplot(data, aes(x = petal_length, y = sepal_length, color = species)) + geom_point()
```

Suppose we do not know labels but want to group observations
based on features.
```{r}
data_features <- select(data, -species)
km <- kmeans(data_features, centers = 3)
km
```

```{r}
data_w_clusters <- mutate(data_features, cluster = factor(km$cluster))

ggplot(data_w_clusters, aes(x = petal_length, y = sepal_length, color = cluster)) +
  geom_point()
```

We can inspect the resulting centers.
```{r}
km$centers
```

```{r}
centers <- as_tibble(km$centers) %>% 
  mutate(cluster = factor(seq(nrow(km$centers))), center = TRUE)
kable(centers, digits = 3) %>%
  kable_styling(full_width = F)
```

```{r}
plot_clusters_with_centers <- function(features, kmeans_object) {

  data_w_clusters <- mutate(features, cluster = factor(kmeans_object$cluster))

  centers <- as_tibble(kmeans_object$centers) %>%
    mutate(cluster = factor(seq(nrow(km$centers))), center = TRUE)

  data_w_clusters_centers <- bind_rows(data_w_clusters, centers)
  ggplot(data_w_clusters_centers, aes(
    x = petal_length, y = sepal_length,
    color = cluster, size = ifelse(!is.na(center), 2, 1))
  ) +
    geom_point() +
    scale_size(guide = 'none')
}

plot_clusters_with_centers(data_features, km)
```

Results depend on the starting centers which are randomly chosen
observations.

```{r}
set.seed(1122)
km <- kmeans(data_features, centers = 3, nstart = 1)
print(km$centers)
print(table(km$cluster))
print(km$withinss)

plot_clusters_with_centers(data_features, km)
```

```{r}
set.seed(223456)
km <- kmeans(data_features, centers = 3, nstart = 1)
print(km$centers)
print(table(km$cluster))
print(km$withinss)

plot_clusters_with_centers(data_features, km)
```

We should always experiment with
different starting values (probably generated randomly).
`nstart` controls how many times the algorithm is run with different
random starting points for the centers. Setting it to a high value
(e.g., 20) is a good idea to achieve the best groupings.

### Choosing K

There are no general rules, depends on the application. There are some
rules of thumb, though. There are many, you can explore some
with explanation, [here](http://www.sthda.com/english/wiki/print.php?id=239).

A popular one looks at the evolution of the within-sum-of-squares, and
identifies the "elbow" point for the optimal number of clusters

```{r}
fviz_nbclust(data_features, kmeans, method = "wss")
```

`NbClust` calculates 30 indices based on various principles and chooses by
majority rule.

```{r, results="hide"}
nb <- NbClust(data_features, method = "kmeans", min.nc = 2, max.nc = 10, index = "all")
nb
```


## Hierarchical clustering

With hierarchical clustering we get a nested structure of clusters
based on a dissimilarity measure. Is it better than k-means? It depends -
k-means does not yield a hierarchical structure. If the data does not have
one in reality, hierarchical may be not as good as k-means and there are
reversed situations as well.

```{r}
data_distances <- dist(data_features)
# use the average distance between groups to decide which
# groups to merge next
hc <- hclust(data_distances, method = "average")
```

```{r}
fviz_dend(hc)
```

```{r}
fviz_dend(hc, k = 3)
```

```{r}
fviz_dend(hc, k = 6)
```

```{r}
# get labels
cluster_hc <- cutree(hc, 3)
data_w_clusters <- mutate(data_features, cluster = factor(cluster_hc))

ggplot(data_w_clusters, aes(x = petal_length, y = sepal_length, color = cluster)) +
  geom_point()
```

How we calculate similarities between groups may strongly affect the clustering.
```{r}
# max distance between points of two groups
hc_complete <- hclust(data_distances, method = "complete")
fviz_dend(hc_complete, k = 3)
```

```{r}
# single: minimal distance between points of two groups
hc_single <- hclust(data_distances, method = "single")
fviz_dend(hc_single, k = 3)
```

## General considerations on clustering

- as both clustering methods are based on distances in features, we may
want to first bring all variables to the same scale
- clusters may be sensitive to details such as scaling or not, whether we take 
subsets of data or what is the dissimilarity measure in hierarchical clustering. 
Advice: experiment with the settings and look for consistent patterns.
