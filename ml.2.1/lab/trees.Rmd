---
title: "Lab week 1 - Tree-based methods"
subtitle: "Data Science and Machine Learning 3 - CEU 2019"
author: "Jeno Pal"
date: '2019-02-18'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, message=FALSE}
library(data.table)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(gbm)
library(ISLR)
library(skimr)
```

## Bagging, random forests

We covered decision trees in previous classes. Using it as a base model lets us
build many different models with less variance and better predictive power.
The downside: interpretation gets harder.

Idea: as individual trees are unstable and have high variance, train many
versions on bootstrap samples ("Bagging": Bootstrap AGGregation).
Then predict: take the average (regression),
majority vote / class share (classification). 

Random forests: randomly constrain the set of predictor variables used to grow trees (randomly select at each split). 
Goal: avoid correlated trees that are very similar to each other,
still with the aim of decreasing variance.

```{r}
data(Hitters)
data <- data.table(Hitters)
skim(data)
```

```{r}
# goal: predict log salary
data <- data[!is.na(Salary)]
data[, log_salary := log(Salary)]
data[, Salary := NULL]
```


```{r}
training_ratio <- 0.75 
set.seed(1234)
train_indices <- createDataPartition(
  y = data[["log_salary"]],
  times = 1,
  p = training_ratio,
  list = FALSE
)

data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

Let's see benchmarks: a linear model and a simple regression tree.
```{r}
train_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3
)

set.seed(857)
linear_model <- train(log_salary ~ .,
                      method = "lm",
                      data = data_train,
                      trControl = train_control)
linear_model
```

```{r}
set.seed(857)
simple_tree_model <- train(log_salary ~ .,
                      method = "rpart",
                      data = data_train,
                      tuneGrid = data.frame(cp = c(0.01, 0.02, 0.05)),
                      trControl = train_control)
simple_tree_model
```

```{r}
rpart.plot(simple_tree_model[["finalModel"]])
```

For random forests,
`mtry` sets the number of variables randomly chosen for any split in the tree. When `mtry`
equals the number of features, it is the bagging.

```{r}
tune_grid <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 12, 19),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)

# random forest
set.seed(857)
rf_model <- train(log_salary ~ .,
                  method = "ranger",
                  data = data_train,
                  trControl = train_control,
                  tuneGrid = tune_grid,
                  importance = "impurity"
                  )
rf_model
```

```{r}
tune_grid <- expand.grid(
  .mtry = c(2, 3, 5),
  .splitrule = "variance",
  .min.node.size = 5
)

# the number of trees is not a tuning parameter with caret
# default is 500, you can change it with passing the parameter to train
set.seed(857)
rf_model_ntree_10 <- train(log_salary ~ .,
                  method = "ranger",
                  data = data_train,
                  trControl = train_control,
                  tuneGrid = tune_grid,
                  num.trees = 10,
                  importance = "impurity"
                  )
rf_model_ntree_10
```

```{r}
set.seed(857)
rf_model_ntree_1000 <- train(
  log_salary ~ .,
  method = "ranger",
  data = data_train,
  trControl = train_control,
  tuneGrid = tune_grid,
  num.trees = 1000,
  importance = "impurity"
)
rf_model_ntree_1000
```

```{r}
resamples(list(rf_model_ntree_10, rf_model, rf_model_ntree_1000)) %>% 
  summary()
```

```{r}
# calculate test error
RMSE(data_test[["log_salary"]], predict.train(rf_model, newdata = data_test))
```

(It is a small dataset hence performance measures can have large
variances.)

### Out of bag error

Bootstrap samples: some observations are used more than once, some not at all.
On average, around 1/3 of the observations are not used in a bootstrap sample.

Idea: for each sample point, obtain a random forest prediction from those trees
where it was not used in the boostrap sample. Then comparing this prediction to reality
gives an honest measure of performance.

```{r}
rf_model$finalModel
```
```{r}
# the out of bag RMSE
sqrt(rf_model$finalModel$prediction.error)
```

We can base our model selection on OOB error instead of CV. It is generally faster
as you do not have to re-estimate the same model many times, you just use the output
of the `num.trees` trees that you estimate for RF.
```{r}
tune_grid <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 12, 19),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)

train_control_oob <- trainControl(method = "oob")

# random forest
set.seed(857)
rf_model_oob <- train(log_salary ~ .,
                  method = "ranger",
                  data = data_train,
                  trControl = train_control_oob,
                  tuneGrid = tune_grid,
                  importance = "impurity"
                  )
rf_model_oob
```

### Variable importance

With the ensemble models we have a hard time with interpretation.
Variable importance measures can help to see which features contribute most
to the predictive power of models. The generic `varImp` function of `caret`
does model-specific calculations, consult [here](https://topepo.github.io/caret/variable-importance.html) for a description
for your model at hand.

```{r}
varImp(rf_model)
```

```{r}
plot(varImp(rf_model))
```

## Gradient boosting machines

Gradient boosting machines: also ensembles of trees, however,
the method of choosing them is different. Idea: get the residual and train
next tree to predict (explain) the residual. Then add it to the previous
trees, with a shrinkage parameter (to avoid overfitting).

Another difference: GBMs use shallow trees (controlled by
`interaction.depth`) whereas RFs use unpruned, large trees (with low bias
and high variance). Common part: idea of bagging.

```{r}
gbm_grid <- expand.grid(n.trees = c(100, 500, 1000), 
                        interaction.depth = c(2, 3, 5), 
                        shrinkage = c(0.005, 0.01, 0.1),
                        n.minobsinnode = c(5))
  
gbm_model <- train(log_salary ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model
```
```{r}
# we can refine the grid around the optimum found
gbm_grid_refined <- expand.grid(
  n.trees = c(500, 1000, 2000), 
  interaction.depth = c(3, 5, 7), 
  shrinkage = c(0.001, 0.005),
  n.minobsinnode = c(5)
)

set.seed(857)
gbm_model_refined <- train(log_salary ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid_refined,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model_refined

```

Lower `eta` means slower learning, hence more trees are necessary to have
good performance.

4 hyperparameters: [the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) 
can make it hard to come up
with efficient grids for hyperparameter search. However, with `gbm` and many
other models, in reality the models to be estimated are not exponentially
growing with the number of hyperparameters (see [here](https://topepo.github.io/caret/random-hyperparameter-search.html)).

The variable importance profile is typically more extreme than for random forests. This is not a contradiction or a problem, just a property of the models and ultimately different faces of the data.
```{r}
plot(varImp(gbm_model))
```

Not tuned: `bag.fraction` parameter (set to default 0.5): for the construction
of each tree, only `bag.fraction` share of the sample is used (randomly
selected, see `?gbm`). 
This, again, is the same idea as with bagging: decrease
variance of the model. You can pass another value for it via
giving `train` an argument `bag.fraction`, just like we saw with
`ranger` and `num.trees`.

```{r}
gbm_grid_2 <- data.frame(n.trees = c(1000), 
                         interaction.depth = c(7), 
                         shrinkage = c(0.005),
                         n.minobsinnode = c(5))

set.seed(857)
gbm_model_2 <- train(log_salary ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid_2,
                   bag.fraction = 0.8,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model_2
```

### XGBoost

A celebrated implementation of the gradient boosting idea. 
_"Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance."_

See documentation [here](http://xgboost.readthedocs.io/).
It proved to be very stable and widely applicable. For the many hyperparameters,
consult the documentation. New ones compared to prevoiusly seen `gbm`:
  * `colsample_bytree`: constrain number of columns to use to build a tree in a step
```{r}
xgb_grid <- expand.grid(nrounds = c(500, 1000),
                       max_depth = c(2, 3, 5),
                       eta = c(0.01, 0.05),
                       gamma = 0,
                       colsample_bytree = c(0.5, 0.7),
                       min_child_weight = 1, # similar to n.minobsinnode
                       subsample = c(0.5))
set.seed(857)
xgboost_model <- train(log_salary ~ .,
                       method = "xgbTree",
                       data = data_train,
                       trControl = train_control,
                       tuneGrid = xgb_grid)
xgboost_model
```

```{r}
plot(varImp(xgboost_model))
```

```{r}
resamples_object <- resamples(list("rpart" = simple_tree_model,
                                   "rf" = rf_model,
                                   "gbm" = gbm_model_refined,
                                   "xgboost" = xgboost_model))
summary(resamples_object)
```




